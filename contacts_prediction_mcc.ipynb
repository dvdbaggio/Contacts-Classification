{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vvk9fWsw2k-c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, balanced_accuracy_score, average_precision_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqve4GHx2ntF",
        "outputId": "d3be2778-bc64-41d2-e72f-8f4dc1b211c7"
      },
      "outputs": [],
      "source": [
        "# CHECK CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0KiFdFm3t4L"
      },
      "source": [
        "# Mount Drive and Load Data\n",
        "\n",
        "To create the training dataset, upload the `features_ring` folder to your personal drive, and update the path accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mQmSILF27p-",
        "outputId": "a485f67e-c48d-403b-f0ad-bab8f5ffc463"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlCnjfzo3Q1M",
        "outputId": "84cd9e9e-7254-45fc-bb83-b82c219c9f71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded cached DataFrame!\n"
          ]
        }
      ],
      "source": [
        "path = \"/Users/sebastianosanson/Development/Contacts-Classification/\"\n",
        "cache_file = os.path.join(path, 'features_ring_df.pkl')\n",
        "\n",
        "if os.path.exists(cache_file):\n",
        "    df = pd.read_pickle(cache_file)\n",
        "    print(\"Loaded cached DataFrame!\")\n",
        "else:\n",
        "    dir = os.path.join(path, 'features_ring')\n",
        "    df = pd.DataFrame()\n",
        "    for file in os.listdir(dir):\n",
        "        if file.endswith('.tsv'):\n",
        "            df_temp = pd.read_csv(os.path.join(dir, file), sep='\\t')\n",
        "            df = pd.concat([df, df_temp])\n",
        "    df.to_pickle(cache_file)\n",
        "    print(\"Processed and saved DataFrame!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qHlyYZ7xetJ2"
      },
      "outputs": [],
      "source": [
        "columns_to_check = df.columns[:-1]\n",
        "df_copy = df.copy()\n",
        "df = df.drop_duplicates(subset=columns_to_check, keep='last')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpwC7Qb0_qqS"
      },
      "source": [
        "## Dataset creation\n",
        "\n",
        "Add the label unclassified, fill with the mean off the column `None` value and encode as integer the secondary structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZI0HRXu4JUO",
        "outputId": "d2aa64a6-aa67-4de5-bd06-0aba1292fec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interaction\n",
            "Unclassified    1089547\n",
            "VDW              737061\n",
            "HBOND            694154\n",
            "PIPISTACK         15091\n",
            "PICATION           3222\n",
            "IONIC              1432\n",
            "SSBOND              729\n",
            "PIHBOND             722\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Labelling None values on column 'Interaction' with a proper label\n",
        "df['Interaction'] = df['Interaction'].fillna('Unclassified')\n",
        "interaction_counts = df['Interaction'].value_counts()\n",
        "print(interaction_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKd3909m8NPR",
        "outputId": "c6db9846-11c1-4e83-9817-d1a976687985"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ts/jhrtqcqd6xl3s86wsbgd3fdc0000gn/T/ipykernel_22084/1592439739.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = df['Interaction'].replace(contact_dict)\n"
          ]
        }
      ],
      "source": [
        "contact_dict = {\n",
        "    \"HBOND\": 0,\n",
        "    \"VDW\": 1,\n",
        "    \"PIPISTACK\": 2,\n",
        "    \"IONIC\": 3,\n",
        "    \"PICATION\": 4,\n",
        "    \"SSBOND\": 5,\n",
        "    \"PIHBOND\": 6,\n",
        "    \"Unclassified\": 7\n",
        "}\n",
        "\n",
        "# Apply the mapping to create numerical labels\n",
        "y = df['Interaction'].replace(contact_dict)\n",
        "X = df[['s_ss8','s_rsa', 's_phi', 's_psi', 's_a1', 's_a2', 's_a3', 's_a4', 's_a5', 's_3di_state', 's_3di_letter',\n",
        "        't_ss8', 't_rsa', 't_phi', 't_psi', 't_a1', 't_a2', 't_a3', 't_a4', 't_a5', 't_3di_state', 't_3di_letter']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JQIaPT2B8VR8"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features\n",
        "le = LabelEncoder()\n",
        "X['s_ss8_encoded'] = le.fit_transform(X['s_ss8'])\n",
        "X['t_ss8_encoded'] = le.fit_transform(X['t_ss8'])\n",
        "X = X.drop(columns=['s_ss8', 't_ss8', 's_3di_letter', 't_3di_letter'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VKHqZHCsmbp",
        "outputId": "0fa63b46-7adc-4397-cbf2-fde307aa6527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            " s_rsa             63\n",
            "s_phi          15355\n",
            "s_psi           5727\n",
            "s_3di_state    31563\n",
            "t_rsa             73\n",
            "t_phi           5556\n",
            "t_psi          18133\n",
            "t_3di_state    37305\n",
            "dtype: int64\n",
            "\n",
            "Total missing values: 113775\n"
          ]
        }
      ],
      "source": [
        "# Count total missing values per column\n",
        "missing_per_column = X.isna().sum()\n",
        "missing_columns = missing_per_column[missing_per_column > 0]\n",
        "print(\"Missing values per column:\\n\", missing_columns)\n",
        "\n",
        "total_missing = X.isna().sum().sum()\n",
        "print(f\"\\nTotal missing values: {total_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thDFe8_FmLmO",
        "outputId": "e3633375-1eef-4f32-a3ae-c91ae863060a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total missing values, after refilling: 0\n",
            "\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   t_phi  t_psi   t_a1   t_a2   t_a3   t_a4   t_a5  t_3di_state  \\\n",
            "0 -2.322  2.373  1.831 -0.561  0.533 -0.277  1.648          6.0   \n",
            "1 -1.788  2.740 -1.343  0.465 -0.862 -1.020 -0.255          4.0   \n",
            "2 -2.390  2.897  0.931 -0.179 -3.005 -0.503 -1.853          2.0   \n",
            "3 -1.088 -0.766 -0.228  1.399 -4.760  0.670 -2.647         17.0   \n",
            "4 -1.551 -0.116  1.357 -1.453  1.477  0.113 -0.837         13.0   \n",
            "\n",
            "   s_ss8_encoded  t_ss8_encoded  \n",
            "0              3              7  \n",
            "1              0              1  \n",
            "2              2              0  \n",
            "3              4              4  \n",
            "4              0              3  \n"
          ]
        }
      ],
      "source": [
        "# Fill None values with the mean of the values of that column\n",
        "X = X.apply(lambda x: x.fillna(x.mean()) if x.dtype.kind in 'biufc' else x)\n",
        "\n",
        "total_missing = X.isna().sum().sum()\n",
        "print(f\"Total missing values, after refilling: {total_missing}\\n\")\n",
        "\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZtIyKWHjOY"
      },
      "source": [
        "## Feature engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTvkv49Vk1MD"
      },
      "source": [
        "*   Sum\n",
        "*   Product\n",
        "*   Absolute difference\n",
        "*   Average\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uuBW0Ngmkf-f"
      },
      "outputs": [],
      "source": [
        "def fe(feature):\n",
        "  print(f'Engineering feature: {feature}')\n",
        "\n",
        "  source_feature = 's_' + feature\n",
        "  target_feature = 't_' + feature\n",
        "\n",
        "  sum_feature = f'{feature}_sum'\n",
        "  abs_diff_feature = f'{feature}_abs_diff'\n",
        "  prod_feature = f'{feature}_prod'\n",
        "  avg_feature = f'{feature}_avg'\n",
        "\n",
        "  list_feature_names = [sum_feature, abs_diff_feature, prod_feature, avg_feature]\n",
        "\n",
        "  X[sum_feature] = X[source_feature] + X[target_feature]\n",
        "  X[abs_diff_feature] = np.abs(X[source_feature] - X[target_feature])\n",
        "  X[prod_feature] = X[source_feature] * X[target_feature]\n",
        "  X[avg_feature] = (X[source_feature] + X[target_feature]) / 2\n",
        "\n",
        "  print(X.head())\n",
        "\n",
        "  return list_feature_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NXKr_Z9nKoT",
        "outputId": "4d31dd20-b09e-470a-b765-9ebc231233ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Engineering feature: ss8_encoded\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...   t_a3   t_a4   t_a5  t_3di_state  s_ss8_encoded  t_ss8_encoded  \\\n",
            "0  ...  0.533 -0.277  1.648          6.0              3              7   \n",
            "1  ... -0.862 -1.020 -0.255          4.0              0              1   \n",
            "2  ... -3.005 -0.503 -1.853          2.0              2              0   \n",
            "3  ... -4.760  0.670 -2.647         17.0              4              4   \n",
            "4  ...  1.477  0.113 -0.837         13.0              0              3   \n",
            "\n",
            "   ss8_encoded_sum  ss8_encoded_abs_diff  ss8_encoded_prod  ss8_encoded_avg  \n",
            "0               10                     4                21              5.0  \n",
            "1                1                     1                 0              0.5  \n",
            "2                2                     2                 0              1.0  \n",
            "3                8                     0                16              4.0  \n",
            "4                3                     3                 0              1.5  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "Engineering feature: rsa\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...  s_ss8_encoded  t_ss8_encoded  ss8_encoded_sum  ss8_encoded_abs_diff  \\\n",
            "0  ...              3              7               10                     4   \n",
            "1  ...              0              1                1                     1   \n",
            "2  ...              2              0                2                     2   \n",
            "3  ...              4              4                8                     0   \n",
            "4  ...              0              3                3                     3   \n",
            "\n",
            "   ss8_encoded_prod  ss8_encoded_avg  rsa_sum  rsa_abs_diff  rsa_prod  rsa_avg  \n",
            "0                21              5.0    1.076         0.188  0.280608    0.538  \n",
            "1                 0              0.5    0.134         0.134  0.000000    0.067  \n",
            "2                 0              1.0    0.298         0.238  0.008040    0.149  \n",
            "3                16              4.0    0.118         0.082  0.001800    0.059  \n",
            "4                 0              1.5    0.614         0.294  0.072640    0.307  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "Engineering feature: phi\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...  ss8_encoded_prod  ss8_encoded_avg  rsa_sum  rsa_abs_diff  rsa_prod  \\\n",
            "0  ...                21              5.0    1.076         0.188  0.280608   \n",
            "1  ...                 0              0.5    0.134         0.134  0.000000   \n",
            "2  ...                 0              1.0    0.298         0.238  0.008040   \n",
            "3  ...                16              4.0    0.118         0.082  0.001800   \n",
            "4  ...                 0              1.5    0.614         0.294  0.072640   \n",
            "\n",
            "   rsa_avg  phi_sum  phi_abs_diff  phi_prod  phi_avg  \n",
            "0    0.538   -3.442         1.202  2.600640  -1.7210  \n",
            "1    0.067   -4.098         0.522  4.130280  -2.0490  \n",
            "2    0.149   -4.235         0.545  4.409550  -2.1175  \n",
            "3    0.059   -2.181         0.005  1.189184  -1.0905  \n",
            "4    0.307   -3.769         0.667  3.440118  -1.8845  \n",
            "\n",
            "[5 rows x 32 columns]\n",
            "Engineering feature: psi\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...  rsa_prod  rsa_avg  phi_sum  phi_abs_diff  phi_prod  phi_avg  psi_sum  \\\n",
            "0  ...  0.280608    0.538   -3.442         1.202  2.600640  -1.7210    2.035   \n",
            "1  ...  0.000000    0.067   -4.098         0.522  4.130280  -2.0490    4.065   \n",
            "2  ...  0.008040    0.149   -4.235         0.545  4.409550  -2.1175    4.914   \n",
            "3  ...  0.001800    0.059   -2.181         0.005  1.189184  -1.0905   -1.668   \n",
            "4  ...  0.072640    0.307   -3.769         0.667  3.440118  -1.8845    2.711   \n",
            "\n",
            "   psi_abs_diff  psi_prod  psi_avg  \n",
            "0         2.711 -0.802074   1.0175  \n",
            "1         1.415  3.630500   2.0325  \n",
            "2         0.880  5.843249   2.4570  \n",
            "3         0.136  0.690932  -0.8340  \n",
            "4         2.943 -0.327932   1.3555  \n",
            "\n",
            "[5 rows x 36 columns]\n",
            "Engineering feature: a1\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...  phi_prod  phi_avg  psi_sum  psi_abs_diff  psi_prod  psi_avg  a1_sum  \\\n",
            "0  ...  2.600640  -1.7210    2.035         2.711 -0.802074   1.0175   1.240   \n",
            "1  ...  4.130280  -2.0490    4.065         1.415  3.630500   2.0325   0.014   \n",
            "2  ...  4.409550  -2.1175    4.914         0.880  5.843249   2.4570  -0.308   \n",
            "3  ...  1.189184  -1.0905   -1.668         0.136  0.690932  -0.8340   0.822   \n",
            "4  ...  3.440118  -1.8845    2.711         2.943 -0.327932   1.3555   0.118   \n",
            "\n",
            "   a1_abs_diff   a1_prod  a1_avg  \n",
            "0        2.422 -1.082121   0.620  \n",
            "1        2.700 -1.822451   0.007  \n",
            "2        2.170 -1.153509  -0.154  \n",
            "3        1.278 -0.239400   0.411  \n",
            "4        2.596 -1.681323   0.059  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "Engineering feature: a2\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...  psi_prod  psi_avg  a1_sum  a1_abs_diff   a1_prod  a1_avg  a2_sum  \\\n",
            "0  ... -0.802074   1.0175   1.240        2.422 -1.082121   0.620  -1.863   \n",
            "1  ...  3.630500   2.0325   0.014        2.700 -1.822451   0.007  -0.988   \n",
            "2  ...  5.843249   2.4570  -0.308        2.170 -1.153509  -0.154  -0.726   \n",
            "3  ...  0.690932  -0.8340   0.822        1.278 -0.239400   0.411   1.701   \n",
            "4  ... -0.327932   1.3555   0.118        2.596 -1.681323   0.059  -2.000   \n",
            "\n",
            "   a2_abs_diff   a2_prod  a2_avg  \n",
            "0        0.741  0.730422 -0.9315  \n",
            "1        1.918 -0.675645 -0.4940  \n",
            "2        0.368  0.097913 -0.3630  \n",
            "3        1.097  0.422498  0.8505  \n",
            "4        0.906  0.794791 -1.0000  \n",
            "\n",
            "[5 rows x 44 columns]\n",
            "Engineering feature: a3\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...   a1_prod  a1_avg  a2_sum  a2_abs_diff   a2_prod  a2_avg  a3_sum  \\\n",
            "0  ... -1.082121   0.620  -1.863        0.741  0.730422 -0.9315  -0.200   \n",
            "1  ... -1.822451   0.007  -0.988        1.918 -0.675645 -0.4940   0.615   \n",
            "2  ... -1.153509  -0.154  -0.726        0.368  0.097913 -0.3630  -0.874   \n",
            "3  ... -0.239400   0.411   1.701        1.097  0.422498  0.8505  -8.416   \n",
            "4  ... -1.681323   0.059  -2.000        0.906  0.794791 -1.0000   3.608   \n",
            "\n",
            "   a3_abs_diff    a3_prod  a3_avg  \n",
            "0        1.266  -0.390689 -0.1000  \n",
            "1        2.339  -1.273174  0.3075  \n",
            "2        5.136  -6.403655 -0.4370  \n",
            "3        1.104  17.402560 -4.2080  \n",
            "4        0.654   3.147487  1.8040  \n",
            "\n",
            "[5 rows x 48 columns]\n",
            "Engineering feature: a4\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...   a2_prod  a2_avg  a3_sum  a3_abs_diff    a3_prod  a3_avg  a4_sum  \\\n",
            "0  ...  0.730422 -0.9315  -0.200        1.266  -0.390689 -0.1000   1.293   \n",
            "1  ... -0.675645 -0.4940   0.615        2.339  -1.273174  0.3075  -0.907   \n",
            "2  ...  0.097913 -0.3630  -0.874        5.136  -6.403655 -0.4370  -0.110   \n",
            "3  ...  0.422498  0.8505  -8.416        1.104  17.402560 -4.2080   0.411   \n",
            "4  ...  0.794791 -1.0000   3.608        0.654   3.147487  1.8040   0.506   \n",
            "\n",
            "   a4_abs_diff   a4_prod  a4_avg  \n",
            "0        1.847 -0.434890  0.6465  \n",
            "1        1.133 -0.115260 -0.4535  \n",
            "2        0.896 -0.197679 -0.0550  \n",
            "3        0.929 -0.173530  0.2055  \n",
            "4        0.280  0.044409  0.2530  \n",
            "\n",
            "[5 rows x 52 columns]\n",
            "Engineering feature: a5\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...    a3_prod  a3_avg  a4_sum  a4_abs_diff   a4_prod  a4_avg  a5_sum  \\\n",
            "0  ...  -0.390689 -0.1000   1.293        1.847 -0.434890  0.6465   1.502   \n",
            "1  ...  -1.273174  0.3075  -0.907        1.133 -0.115260 -0.4535  -1.092   \n",
            "2  ...  -6.403655 -0.4370  -0.110        0.896 -0.197679 -0.0550  -1.037   \n",
            "3  ...  17.402560 -4.2080   0.411        0.929 -0.173530  0.2055  -5.889   \n",
            "4  ...   3.147487  1.8040   0.506        0.280  0.044409  0.2530  -0.021   \n",
            "\n",
            "   a5_abs_diff   a5_prod  a5_avg  \n",
            "0        1.794 -0.240608  0.7510  \n",
            "1        0.582  0.213435 -0.5460  \n",
            "2        2.669 -1.512048 -0.5185  \n",
            "3        0.595  8.581574 -2.9445  \n",
            "4        1.653 -0.682992 -0.0105  \n",
            "\n",
            "[5 rows x 56 columns]\n",
            "Engineering feature: 3di_state\n",
            "   s_rsa  s_phi  s_psi   s_a1   s_a2   s_a3   s_a4   s_a5  s_3di_state  t_rsa  \\\n",
            "0  0.632 -1.120 -0.338 -0.591 -1.302 -0.733  1.570 -0.146         17.0  0.444   \n",
            "1  0.134 -2.310  1.325  1.357 -1.453  1.477  0.113 -0.837          0.0  0.000   \n",
            "2  0.030 -1.845  2.017 -1.239 -0.547  2.131  0.393  0.816          4.0  0.268   \n",
            "3  0.018 -1.093 -0.902  1.050  0.302 -3.656 -0.259 -3.242         17.0  0.100   \n",
            "4  0.160 -2.218  2.827 -1.239 -0.547  2.131  0.393  0.816          6.0  0.454   \n",
            "\n",
            "   ...   a4_prod  a4_avg  a5_sum  a5_abs_diff   a5_prod  a5_avg  \\\n",
            "0  ... -0.434890  0.6465   1.502        1.794 -0.240608  0.7510   \n",
            "1  ... -0.115260 -0.4535  -1.092        0.582  0.213435 -0.5460   \n",
            "2  ... -0.197679 -0.0550  -1.037        2.669 -1.512048 -0.5185   \n",
            "3  ... -0.173530  0.2055  -5.889        0.595  8.581574 -2.9445   \n",
            "4  ...  0.044409  0.2530  -0.021        1.653 -0.682992 -0.0105   \n",
            "\n",
            "   3di_state_sum  3di_state_abs_diff  3di_state_prod  3di_state_avg  \n",
            "0           23.0                11.0           102.0           11.5  \n",
            "1            4.0                 4.0             0.0            2.0  \n",
            "2            6.0                 2.0             8.0            3.0  \n",
            "3           34.0                 0.0           289.0           17.0  \n",
            "4           19.0                 7.0            78.0            9.5  \n",
            "\n",
            "[5 rows x 60 columns]\n"
          ]
        }
      ],
      "source": [
        "features = ['ss8_encoded','rsa', 'phi', 'psi', 'a1', 'a2', 'a3', 'a4', 'a5', '3di_state']\n",
        "# UPDATE WITH NEW ENGINEERED FEATURES\n",
        "feature_names = [\n",
        "      's_ss8','s_rsa', 's_phi', 's_psi', 's_a1', 's_a2', 's_a3', 's_a4', 's_a5', 's_3di_state',\n",
        "      't_ss8', 't_rsa', 't_phi', 't_psi', 't_a1', 't_a2', 't_a3', 't_a4', 't_a5', 't_3di_state',]\n",
        "\n",
        "for feature in features:\n",
        "  feature_names.extend(fe(feature))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxwk5iL808yY",
        "outputId": "842c973b-3773-45bd-bcd0-8b40c81d118e"
      },
      "outputs": [],
      "source": [
        "print(feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkYaddHlcTxB"
      },
      "source": [
        "## Scaling features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gmeFE-sB8ik1"
      },
      "outputs": [],
      "source": [
        "# Scale all features to the range [0, 1]\n",
        "minmax = MinMaxScaler()\n",
        "X_scaled = minmax.fit_transform(X)\n",
        "input_dim = X_scaled.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f1wAsZMbGtb-"
      },
      "outputs": [],
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_scaled, y,\n",
        "    stratify=y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    stratify=y_train_val,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert data to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC_IzZzN-RCS"
      },
      "source": [
        "# SMOTE Oversampling\n",
        "## Choose whether to run SMOTE from scratch (time-consuming) or load the provided `.npy` files containing a precomputed SMOTE run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpSuRISl_EJv"
      },
      "source": [
        "## 1 - Run SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD7lJQMf-fqb",
        "outputId": "a66895d3-3e66-47b0-9664-f8289ec0d9cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: 444258\n",
            "1: 471719\n",
            "2: 9658\n",
            "3: 917\n",
            "4: 2062\n",
            "5: 466\n",
            "6: 463\n",
            "7: 697309\n"
          ]
        }
      ],
      "source": [
        "class_distribution = Counter(y_train)\n",
        "for label in sorted(class_distribution):\n",
        "    print(f\"{label}: {class_distribution[label]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bCsNjPW-oHd"
      },
      "outputs": [],
      "source": [
        "sampling_strategy = {\n",
        "    0: 444258,  # HBOND\n",
        "    1: 471719,  # VDW\n",
        "    2: 24501,  # PIPISTACK\n",
        "    3: 10000,  # IONIC\n",
        "    4: 10000,  # PICATION\n",
        "    5: 5000,  # SSBOND\n",
        "    6: 5000,  # PIHBOND\n",
        "    7: 697310   # Unclassified\n",
        "}\n",
        "\n",
        "oversample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
        "\n",
        "# Fit and resample the training data\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Verify the resampled data\n",
        "print('\\nResampled y_train_bal distribution')\n",
        "for label in sorted(Counter(y_train)):\n",
        "    print(f\"{label}: {Counter(y_train)[label]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8fzU8HGlIC6"
      },
      "outputs": [],
      "source": [
        "np.save('X_bal.npy', X_train)\n",
        "np.save('y_bal.npy', y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CurAq5HD0D5"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVHJfZ2tO0VR"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeBnnZXla1Dt"
      },
      "outputs": [],
      "source": [
        "def feature_importance(model, feature_names):\n",
        "  # Estrai le importanze delle feature\n",
        "  importance = model.get_score(importance_type='weight')  # 'weight', 'gain', or 'cover'\n",
        "\n",
        "  # Ordinare le feature per importanza\n",
        "  # Create a mapping from old keys to new feature names\n",
        "  key_mapping = {f'f{i}': feature_names[i] for i in range(len(feature_names))}\n",
        "\n",
        "  # Replace keys in the importance dictionary\n",
        "  mapped_importance = {key_mapping.get(key, key): value for key, value in importance.items()}\n",
        "\n",
        "  # Sort the features by importance\n",
        "  sorted_importance = sorted(mapped_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "  features, scores = zip(*sorted_importance)\n",
        "\n",
        "  # Visualizza l'importanza delle feature\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.barh(features, scores)\n",
        "  plt.xlabel('Importance Score')\n",
        "  plt.title('Feature Importance')\n",
        "  plt.gca().invert_yaxis()  # Per visualizzare la feature piÃ¹ importante in cima\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0deuc95Kyzz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, feature_names):\n",
        "    \"\"\"\n",
        "    Evaluate performance of an XGBoost model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_path: str\n",
        "        Path to the model file\n",
        "    X_test: numpy array\n",
        "        Test features\n",
        "    y_test: numpy array\n",
        "        Test labels\n",
        "    \"\"\"\n",
        "    # Convert test data to DMatrix\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    # Get predictions from the model\n",
        "    start_time = time.time()\n",
        "    y_pred_prob = model.predict(dtest)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Metric': [\n",
        "            'Accuracy',\n",
        "            'Balanced Accuracy',\n",
        "            'AUC-ROC',\n",
        "            'Matthews Correlation',\n",
        "            'Average Precision',\n",
        "            'Inference Time (ms)',\n",
        "        ],\n",
        "        'Value': [\n",
        "            accuracy_score(y_test, y_pred),\n",
        "            balanced_accuracy_score(y_test, y_pred),\n",
        "            roc_auc_score(y_test, y_pred_prob, multi_class='ovr'),\n",
        "            matthews_corrcoef(y_test, y_pred),\n",
        "            average_precision_score(y_test, y_pred_prob),\n",
        "            inference_time * 1000,\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame for metrics\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "    print(f\"\\n===== Performance Metrics =====\")\n",
        "    print(metrics_df.set_index('Metric').round(4))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    labels = sorted(contact_dict.keys(), key=lambda x: contact_dict[x])  # Sort by dict values\n",
        "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n===== Feature Importance =====\")\n",
        "\n",
        "    # 8. Feature importance\n",
        "    feature_importance(model, feature_names)\n",
        "\n",
        "    return metrics_df, y_pred_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYjlXkh8O4hw"
      },
      "source": [
        "### Train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRcDIHINM3S1"
      },
      "outputs": [],
      "source": [
        "# Create DMatrix objects\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "# Train model\n",
        "model = xgb.train(\n",
        "    params = {\n",
        "        'device': 'cuda',\n",
        "        'seed': 42,\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': len(np.unique(y)),\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'max_depth': 10,\n",
        "        'learning_rate': 0.1,\n",
        "    },\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=2500,\n",
        "    evals=[(dval, 'mlogloss')],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose_eval=100\n",
        ")\n",
        "\n",
        "# Save model\n",
        "os.makedirs(os.path.join(path, 'models'), exist_ok=True)\n",
        "model.save_model(os.path.join(path, f'models/xgboost_model_mcc.json'))\n",
        "\n",
        "evaluate_model(model, X_test, y_test, feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVwjLxdZaLl4"
      },
      "source": [
        "## Pruning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pEsbHa9Kyzz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_xgboost_model(model_path):\n",
        "    \"\"\"Analyze an XGBoost model JSON file and extract useful information.\"\"\"\n",
        "    with open(model_path, 'r') as f:\n",
        "        model_data = json.load(f)\n",
        "\n",
        "    # Extract basic model information\n",
        "    results = {}\n",
        "\n",
        "    # Model metadata and version\n",
        "    results['version'] = '.'.join(map(str, model_data.get('learner', {}).get('version', [\"unknown\"])))\n",
        "\n",
        "    # Model attributes (parameters)\n",
        "    attributes = model_data.get('learner', {}).get('attributes', {})\n",
        "    results['best_iteration'] = int(attributes.get('best_iteration', -1))\n",
        "    results['best_score'] = float(attributes.get('best_score', -1))\n",
        "\n",
        "    # Extract hyperparameters\n",
        "    for key in ['max_depth', 'learning_rate', 'objective', 'eval_metric', 'num_class',\n",
        "                'scale_pos_weight', 'seed', 'num_parallel_tree', 'subsample', 'colsample_bytree']:\n",
        "        if key in attributes:\n",
        "            try:\n",
        "                value = attributes[key]\n",
        "                # Convert numeric strings to proper types\n",
        "                if isinstance(value, str) and value.replace('.', '', 1).isdigit():\n",
        "                    if '.' in value:\n",
        "                        results[key] = float(value)\n",
        "                    else:\n",
        "                        results[key] = int(value)\n",
        "                else:\n",
        "                    results[key] = value\n",
        "            except:\n",
        "                results[key] = attributes[key]\n",
        "\n",
        "    # Extract tree information\n",
        "    tree_model = model_data.get('learner', {}).get('gradient_booster', {}).get('model', {})\n",
        "\n",
        "    # Tree model parameters\n",
        "    gbtree_params = tree_model.get('gbtree_model_param', {})\n",
        "    results['num_trees'] = int(gbtree_params.get('num_trees', 0))\n",
        "\n",
        "    # Trees analysis\n",
        "    trees = tree_model.get('trees', [])\n",
        "\n",
        "    # Tree statistics\n",
        "    tree_stats = []\n",
        "    total_nodes = 0\n",
        "    total_leaves = 0\n",
        "    used_features = set()\n",
        "    feature_importance = defaultdict(int)\n",
        "\n",
        "    for i, tree in enumerate(trees):\n",
        "        tree_param = tree.get('tree_param', {})\n",
        "        num_nodes = int(tree_param.get('num_nodes', 0))\n",
        "        split_indices = tree.get('split_indices', [])\n",
        "\n",
        "        # Calculate leaves (nodes that are not internal nodes)\n",
        "        internal_nodes = set()\n",
        "        for j in range(len(tree.get('left_children', []))):\n",
        "            if tree['left_children'][j] >= 0:  # Not a leaf\n",
        "                internal_nodes.add(j)\n",
        "            if tree['right_children'][j] >= 0:  # Not a leaf\n",
        "                internal_nodes.add(j)\n",
        "\n",
        "        num_leaves = num_nodes - len(internal_nodes)\n",
        "\n",
        "        # Track feature usage as a simple feature importance\n",
        "        for feature_idx in split_indices:\n",
        "            used_features.add(feature_idx)\n",
        "            feature_importance[feature_idx] += 1\n",
        "\n",
        "        # Store tree statistics\n",
        "        tree_stats.append({\n",
        "            'tree_index': i,\n",
        "            'num_nodes': num_nodes,\n",
        "            'num_leaves': num_leaves,\n",
        "            'max_depth': max_tree_depth(tree),\n",
        "            'num_features_used': len(set(split_indices))\n",
        "        })\n",
        "\n",
        "        total_nodes += num_nodes\n",
        "        total_leaves += num_leaves\n",
        "\n",
        "    results['total_nodes'] = total_nodes\n",
        "    results['total_leaves'] = total_leaves\n",
        "    results['avg_nodes_per_tree'] = total_nodes / results['num_trees'] if results['num_trees'] > 0 else 0\n",
        "    results['avg_leaves_per_tree'] = total_leaves / results['num_trees'] if results['num_trees'] > 0 else 0\n",
        "    results['num_features_used'] = len(used_features)\n",
        "\n",
        "    # Sort feature importance\n",
        "    results['top_features'] = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    # Compute tree depth statistics\n",
        "    depths = [stat['max_depth'] for stat in tree_stats]\n",
        "    results['min_tree_depth'] = min(depths) if depths else 0\n",
        "    results['max_tree_depth'] = max(depths) if depths else 0\n",
        "    results['avg_tree_depth'] = sum(depths) / len(depths) if depths else 0\n",
        "\n",
        "    # File metrics\n",
        "    results['file_size_kb'] = os.path.getsize(model_path) / 1024\n",
        "\n",
        "    return results, tree_stats\n",
        "\n",
        "def max_tree_depth(tree):\n",
        "    \"\"\"Calculate the maximum depth of a tree by following child nodes.\"\"\"\n",
        "    left = tree.get('left_children', [])\n",
        "    right = tree.get('right_children', [])\n",
        "\n",
        "    if not left or not right:\n",
        "        return 0\n",
        "\n",
        "    # Use BFS to find the maximum depth\n",
        "    depths = {0: 0}  # node_id: depth\n",
        "    max_depth = 0\n",
        "\n",
        "    for node_id in range(len(left)):\n",
        "        node_depth = depths.get(node_id, 0)\n",
        "\n",
        "        # Process left child\n",
        "        if left[node_id] >= 0:  # Valid node\n",
        "            depths[left[node_id]] = node_depth + 1\n",
        "            max_depth = max(max_depth, node_depth + 1)\n",
        "\n",
        "        # Process right child\n",
        "        if right[node_id] >= 0:  # Valid node\n",
        "            depths[right[node_id]] = node_depth + 1\n",
        "            max_depth = max(max_depth, node_depth + 1)\n",
        "\n",
        "    return max_depth\n",
        "\n",
        "# Usage example\n",
        "models_dir = '/Users/sebastianosanson/Development/Contacts-Classification/models'\n",
        "all_model_results = []\n",
        "all_tree_stats = []\n",
        "\n",
        "# Process all model files\n",
        "for filename in sorted(os.listdir(models_dir)):\n",
        "    if filename.endswith('.json'):\n",
        "        model_path = os.path.join(models_dir, filename)\n",
        "        class_num = int(filename.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Extract model information\n",
        "        model_info, tree_stats = analyze_xgboost_model(model_path)\n",
        "        model_info['class'] = class_num\n",
        "\n",
        "        # Add model information to results\n",
        "        all_model_results.append(model_info)\n",
        "\n",
        "        # Add tree statistics with model identifier\n",
        "        for stat in tree_stats:\n",
        "            stat['class'] = class_num\n",
        "            all_tree_stats.append(stat)\n",
        "\n",
        "# Create DataFrame for easy analysis\n",
        "models_df = pd.DataFrame(all_model_results)\n",
        "trees_df = pd.DataFrame(all_tree_stats)\n",
        "\n",
        "# Print the high-level model information\n",
        "print(\"\\n===== MODEL INFORMATION =====\")\n",
        "print(models_df[['class', 'num_trees', 'best_iteration', 'best_score',\n",
        "                 'total_nodes', 'avg_nodes_per_tree', 'avg_tree_depth',\n",
        "                 'file_size_kb']].sort_values('class'))\n",
        "\n",
        "# Print tree depth statistics\n",
        "print(\"\\n===== TREE DEPTH STATISTICS =====\")\n",
        "tree_depth_stats = trees_df.groupby('class').agg({\n",
        "    'max_depth': ['min', 'max', 'mean']\n",
        "}).reset_index()\n",
        "print(tree_depth_stats)\n",
        "\n",
        "# Feature importance across models\n",
        "print(\"\\n===== TOP FEATURES BY CLASS =====\")\n",
        "for i, model in enumerate(all_model_results):\n",
        "    print(f\"\\nClass {model['class']} top features:\")\n",
        "    for feature_idx, count in model['top_features'][:]:\n",
        "        print(f\"  Feature {feature_names[feature_idx]}: used {count} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfKTs969Kyz0"
      },
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
        "# from sklearn.metrics import matthews_corrcoef, average_precision_score\n",
        "# import matplotlib.pyplot as plt\n",
        "# import time\n",
        "\n",
        "# def evaluate_model_comparison(original_model_path, pruned_model_path, X_test, y_test, class_num):\n",
        "#     \"\"\"\n",
        "#     Compare performance between original and pruned XGBoost models.\n",
        "\n",
        "#     Parameters:\n",
        "#     -----------\n",
        "#     original_model_path: str\n",
        "#         Path to the original model file\n",
        "#     pruned_model_path: str\n",
        "#         Path to the pruned model file\n",
        "#     X_test: numpy array\n",
        "#         Test features\n",
        "#     y_test: numpy array\n",
        "#         Test labels\n",
        "#     class_num: int\n",
        "#         Class number for binary evaluation\n",
        "#     \"\"\"\n",
        "#     # 1. Load both models\n",
        "#     original_model = xgb.Booster()\n",
        "#     original_model.load_model(original_model_path)\n",
        "\n",
        "#     pruned_model = xgb.Booster()\n",
        "#     pruned_model.load_model(pruned_model_path)\n",
        "\n",
        "#     # 2. Convert test data to DMatrix\n",
        "#     dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "#     # 3. Make predictions\n",
        "#     # Convert to binary task (1 for current class, 0 for other classes)\n",
        "#     y_test_binary = (y_test == class_num).astype(int)\n",
        "\n",
        "#     # Get predictions from both models\n",
        "#     start_time = time.time()\n",
        "#     y_pred_orig = original_model.predict(dtest)\n",
        "#     orig_time = time.time() - start_time\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     y_pred_pruned = pruned_model.predict(dtest)\n",
        "#     pruned_time = time.time() - start_time\n",
        "\n",
        "#     # 4. Convert probabilities to binary predictions\n",
        "#     y_pred_orig_binary = (y_pred_orig > 0.5).astype(int)\n",
        "#     y_pred_pruned_binary = (y_pred_pruned > 0.5).astype(int)\n",
        "\n",
        "#     # 5. Calculate metrics\n",
        "#     metrics = {\n",
        "#         'Model': ['Original', 'Pruned'],\n",
        "#         'Accuracy': [\n",
        "#             accuracy_score(y_test_binary, y_pred_orig_binary),\n",
        "#             accuracy_score(y_test_binary, y_pred_pruned_binary)\n",
        "#         ],\n",
        "#         'Balanced Accuracy': [\n",
        "#             balanced_accuracy_score(y_test_binary, y_pred_orig_binary),\n",
        "#             balanced_accuracy_score(y_test_binary, y_pred_pruned_binary)\n",
        "#         ],\n",
        "#         'AUC-ROC': [\n",
        "#             roc_auc_score(y_test_binary, y_pred_orig),\n",
        "#             roc_auc_score(y_test_binary, y_pred_pruned)\n",
        "#         ],\n",
        "#         'Matthews Correlation': [\n",
        "#             matthews_corrcoef(y_test_binary, y_pred_orig_binary),\n",
        "#             matthews_corrcoef(y_test_binary, y_pred_pruned_binary)\n",
        "#         ],\n",
        "#         'Average Precision': [\n",
        "#             average_precision_score(y_test_binary, y_pred_orig),\n",
        "#             average_precision_score(y_test_binary, y_pred_pruned)\n",
        "#         ],\n",
        "#         'Inference Time (ms)': [\n",
        "#             orig_time * 1000,\n",
        "#             pruned_time * 1000\n",
        "#         ]\n",
        "#     }\n",
        "\n",
        "#     # Calculate file size\n",
        "#     import os\n",
        "#     metrics['Model Size (KB)'] = [\n",
        "#         os.path.getsize(original_model_path) / 1024,\n",
        "#         os.path.getsize(pruned_model_path) / 1024\n",
        "#     ]\n",
        "\n",
        "#     # 6. Create a DataFrame for metrics\n",
        "#     metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "#     # 7. Calculate performance difference\n",
        "#     diff_row = {\n",
        "#         'Model': 'Difference (%)',\n",
        "#         'Accuracy': (metrics['Accuracy'][1] - metrics['Accuracy'][0]) / metrics['Accuracy'][0] * 100,\n",
        "#         'Balanced Accuracy': (metrics['Balanced Accuracy'][1] - metrics['Balanced Accuracy'][0]) / metrics['Balanced Accuracy'][0] * 100,\n",
        "#         'AUC-ROC': (metrics['AUC-ROC'][1] - metrics['AUC-ROC'][0]) / metrics['AUC-ROC'][0] * 100,\n",
        "#         'Matthews Correlation': (metrics['Matthews Correlation'][1] - metrics['Matthews Correlation'][0]) / max(0.0001, metrics['Matthews Correlation'][0]) * 100,\n",
        "#         'Average Precision': (metrics['Average Precision'][1] - metrics['Average Precision'][0]) / metrics['Average Precision'][0] * 100,\n",
        "#         'Inference Time (ms)': (metrics['Inference Time (ms)'][1] - metrics['Inference Time (ms)'][0]) / metrics['Inference Time (ms)'][0] * 100,\n",
        "#         'Model Size (KB)': (metrics['Model Size (KB)'][1] - metrics['Model Size (KB)'][0]) / metrics['Model Size (KB)'][0] * 100\n",
        "#     }\n",
        "#     metrics_df = pd.concat([metrics_df, pd.DataFrame([diff_row])], ignore_index=True)\n",
        "\n",
        "#     print(f\"\\n===== Performance Comparison for Class {class_num} =====\")\n",
        "#     print(metrics_df.round(4))\n",
        "\n",
        "#     # 8. Plot prediction correlation\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.scatter(y_pred_orig, y_pred_pruned, alpha=0.3)\n",
        "#     plt.plot([0, 1], [0, 1], 'r--')\n",
        "#     plt.xlabel('Original Model Predictions')\n",
        "#     plt.ylabel('Pruned Model Predictions')\n",
        "#     plt.title(f'Prediction Correlation for Class {class_num}')\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "#     correlation = np.corrcoef(y_pred_orig, y_pred_pruned)[0, 1]\n",
        "#     plt.text(0.05, 0.95, f'Correlation: {correlation:.4f}', transform=plt.gca().transAxes)\n",
        "#     plt.show()\n",
        "\n",
        "#     return metrics_df\n",
        "\n",
        "# # Example usage\n",
        "# # class_num = 0  # For HBOND class\n",
        "# # original_path = '/Users/sebastianosanson/Development/Contacts-Classification/models/xgboost_model_class_0.json'\n",
        "# # pruned_path = '/Users/sebastianosanson/Development/Contacts-Classification/models/xgboost_model_class_0_pruned.json'\n",
        "# # results = evaluate_model_comparison(original_path, pruned_path, X_test, y_test, class_num)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sb_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
