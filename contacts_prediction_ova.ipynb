{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvk9fWsw2k-c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, balanced_accuracy_score, average_precision_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqve4GHx2ntF",
        "outputId": "4caa72dc-17dd-44ae-bc22-63fa78e09ee6"
      },
      "outputs": [],
      "source": [
        "# CHECK CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0KiFdFm3t4L"
      },
      "source": [
        "# Mount Drive and Load Data\n",
        "\n",
        "To create the training dataset, upload the `features_ring` folder to your personal drive, and update the path accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mQmSILF27p-",
        "outputId": "fe145907-cde8-47e6-e40a-348af2568046"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = 'drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlCnjfzo3Q1M",
        "outputId": "df17a694-791f-43a9-bde6-6164f7df7afb"
      },
      "outputs": [],
      "source": [
        "# path = \"/Users/sebastianosanson/Development/Contacts-Classification/\"\n",
        "cache_file = os.path.join(path, 'features_ring_df.pkl')\n",
        "\n",
        "if os.path.exists(cache_file):\n",
        "    df = pd.read_pickle(cache_file)\n",
        "    print(\"Loaded cached DataFrame!\")\n",
        "else:\n",
        "    dir = os.path.join(path, 'features_ring')\n",
        "    df = pd.DataFrame()\n",
        "    for file in os.listdir(dir):\n",
        "        if file.endswith('.tsv'):\n",
        "            df_temp = pd.read_csv(os.path.join(dir, file), sep='\\t')\n",
        "            df = pd.concat([df, df_temp])\n",
        "    df.to_pickle(cache_file)\n",
        "    print(\"Processed and saved DataFrame!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpwC7Qb0_qqS"
      },
      "source": [
        "## Dataset creation\n",
        "\n",
        "Add the label unclassified, fill with the mean off the column `None` value and encode as integer the secondary structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZI0HRXu4JUO",
        "outputId": "1dccf0ea-3911-4bdd-ef69-b263f11cf583"
      },
      "outputs": [],
      "source": [
        "# Labelling None values on column 'Interaction' with a proper label\n",
        "df['Interaction'] = df['Interaction'].fillna('Unclassified')\n",
        "interaction_counts = df['Interaction'].value_counts()\n",
        "print(interaction_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKd3909m8NPR",
        "outputId": "d2044b22-f8f4-4d8d-e1bc-9aeca9fb7cb6"
      },
      "outputs": [],
      "source": [
        "contact_dict = {\n",
        "    \"HBOND\": 0,\n",
        "    \"VDW\": 1,\n",
        "    \"PIPISTACK\": 2,\n",
        "    \"IONIC\": 3,\n",
        "    \"PICATION\": 4,\n",
        "    \"SSBOND\": 5,\n",
        "    \"PIHBOND\": 6,\n",
        "    \"Unclassified\": 7\n",
        "}\n",
        "\n",
        "# Apply the mapping to create numerical labels\n",
        "y = df['Interaction'].replace(contact_dict)\n",
        "X = df[['s_ss8','s_rsa', 's_phi', 's_psi', 's_a1', 's_a2', 's_a3', 's_a4', 's_a5', 's_3di_state', 's_3di_letter',\n",
        "        't_ss8', 't_rsa', 't_phi', 't_psi', 't_a1', 't_a2', 't_a3', 't_a4', 't_a5', 't_3di_state', 't_3di_letter']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQIaPT2B8VR8"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features\n",
        "le = LabelEncoder()\n",
        "X['s_ss8_encoded'] = le.fit_transform(X['s_ss8'])\n",
        "X['t_ss8_encoded'] = le.fit_transform(X['t_ss8'])\n",
        "X = X.drop(columns=['s_ss8', 't_ss8', 's_3di_letter', 't_3di_letter'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VKHqZHCsmbp",
        "outputId": "7472c117-9ad8-4a35-925c-c1c1d37db34a"
      },
      "outputs": [],
      "source": [
        "# Count total missing values per column\n",
        "missing_per_column = X.isna().sum()\n",
        "missing_columns = missing_per_column[missing_per_column > 0]\n",
        "print(\"Missing values per column:\\n\", missing_columns)\n",
        "\n",
        "total_missing = X.isna().sum().sum()\n",
        "print(f\"\\nTotal missing values: {total_missing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thDFe8_FmLmO",
        "outputId": "8a582932-9696-478d-d737-8bb7850589ca"
      },
      "outputs": [],
      "source": [
        "# Fill None values with the mean of the values of that column\n",
        "X = X.apply(lambda x: x.fillna(x.mean()) if x.dtype.kind in 'biufc' else x)\n",
        "\n",
        "total_missing = X.isna().sum().sum()\n",
        "print(f\"Total missing values, after refilling: {total_missing}\\n\")\n",
        "\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZtIyKWHjOY"
      },
      "source": [
        "## Feature engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTvkv49Vk1MD"
      },
      "source": [
        "*   Sum\n",
        "*   Product\n",
        "*   Absolute difference\n",
        "*   Average\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuBW0Ngmkf-f"
      },
      "outputs": [],
      "source": [
        "def fe(feature):\n",
        "  print(f'Engineering feature: {feature}')\n",
        "\n",
        "  source_feature = 's_' + feature\n",
        "  target_feature = 't_' + feature\n",
        "\n",
        "  sum_feature = f'{feature}_sum'\n",
        "  abs_diff_feature = f'{feature}_abs_diff'\n",
        "  prod_feature = f'{feature}_prod'\n",
        "  avg_feature = f'{feature}_avg'\n",
        "\n",
        "  list_feature_names = [sum_feature, abs_diff_feature, prod_feature, avg_feature]\n",
        "\n",
        "  X[sum_feature] = X[source_feature] + X[target_feature]\n",
        "  X[abs_diff_feature] = np.abs(X[source_feature] - X[target_feature])\n",
        "  X[prod_feature] = X[source_feature] * X[target_feature]\n",
        "  X[avg_feature] = (X[source_feature] + X[target_feature]) / 2\n",
        "\n",
        "  print(X.head())\n",
        "\n",
        "  return list_feature_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NXKr_Z9nKoT",
        "outputId": "ea89a321-2ce5-4ea9-8caf-0d5409ac8cad"
      },
      "outputs": [],
      "source": [
        "features = ['ss8_encoded','rsa', 'phi', 'psi', 'a1', 'a2', 'a3', 'a4', 'a5', '3di_state']\n",
        "# UPDATE WITH NEW ENGINEERED FEATURES\n",
        "feature_names = [\n",
        "      's_ss8','s_rsa', 's_phi', 's_psi', 's_a1', 's_a2', 's_a3', 's_a4', 's_a5', 's_3di_state',\n",
        "      't_ss8', 't_rsa', 't_phi', 't_psi', 't_a1', 't_a2', 't_a3', 't_a4', 't_a5', 't_3di_state',]\n",
        "\n",
        "for feature in features:\n",
        "  feature_names.extend(fe(feature))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxwk5iL808yY",
        "outputId": "73f5a1b5-a0bc-46f8-f7e2-d634d7c92ecc"
      },
      "outputs": [],
      "source": [
        "print(feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkYaddHlcTxB"
      },
      "source": [
        "## Scaling features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmeFE-sB8ik1"
      },
      "outputs": [],
      "source": [
        "# Scale all features to the range [0, 1]\n",
        "minmax = MinMaxScaler()\n",
        "X_scaled = minmax.fit_transform(X)\n",
        "input_dim = X_scaled.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1wAsZMbGtb-"
      },
      "outputs": [],
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_scaled, y,\n",
        "    stratify=y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    stratify=y_train_val,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert data to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC_IzZzN-RCS"
      },
      "source": [
        "# SMOTE Oversampling\n",
        "## Choose whether to run SMOTE from scratch (time-consuming) or load the provided `.npy` files containing a precomputed SMOTE run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpSuRISl_EJv"
      },
      "source": [
        "## 1 - Run SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD7lJQMf-fqb",
        "outputId": "57460f2c-34b7-4c07-f3c9-d455796acaba"
      },
      "outputs": [],
      "source": [
        "class_distribution = Counter(y_train)\n",
        "for label in sorted(class_distribution):\n",
        "    print(f\"{label}: {class_distribution[label]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bCsNjPW-oHd",
        "outputId": "0004be18-4d8e-42d9-8c0a-d398ac4ed53a"
      },
      "outputs": [],
      "source": [
        "sampling_strategy = {\n",
        "    0: 675794,  # HBOND\n",
        "    1: 471719,  # VDW\n",
        "    2: 24501,  # PIPISTACK\n",
        "    3: 22650,  # IONIC\n",
        "    4: 20000,  # PICATION\n",
        "    5: 10000,  # SSBOND\n",
        "    6: 10000,  # PIHBOND\n",
        "    7: 697310   # Unclassified\n",
        "}\n",
        "\n",
        "oversample = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
        "\n",
        "# Fit and resample the training data\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Verify the resampled data\n",
        "print('\\nResampled y_train_bal distribution')\n",
        "for label in sorted(Counter(y_train)):\n",
        "    print(f\"{label}: {Counter(y_train)[label]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CurAq5HD0D5"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVHJfZ2tO0VR"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeBnnZXla1Dt"
      },
      "outputs": [],
      "source": [
        "def feature_importance(model, interaction_type, feature_names):\n",
        "  # Estrai le importanze delle feature\n",
        "  importance = model.get_score(importance_type='weight')  # 'weight', 'gain', or 'cover'\n",
        "\n",
        "  # Ordinare le feature per importanza\n",
        "  # Create a mapping from old keys to new feature names\n",
        "  key_mapping = {f'f{i}': feature_names[i] for i in range(len(feature_names))}\n",
        "\n",
        "  # Replace keys in the importance dictionary\n",
        "  mapped_importance = {key_mapping.get(key, key): value for key, value in importance.items()}\n",
        "\n",
        "  # Sort the features by importance\n",
        "  sorted_importance = sorted(mapped_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "  features, scores = zip(*sorted_importance)\n",
        "\n",
        "  # Visualizza l'importanza delle feature\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.barh(features, scores)\n",
        "  plt.xlabel('Importance Score')\n",
        "  plt.title('Feature Importance for ' + str(interaction_type) + ' interaction')\n",
        "  plt.gca().invert_yaxis()  # Per visualizzare la feature più importante in cima\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0deuc95Kyzz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, class_num, feature_names):\n",
        "    \"\"\"\n",
        "    Evaluate performance of an XGBoost model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model_path: str\n",
        "        Path to the model file\n",
        "    X_test: numpy array\n",
        "        Test features\n",
        "    y_test: numpy array\n",
        "        Test labels\n",
        "    class_num: int\n",
        "        Class number for binary evaluation\n",
        "    \"\"\"\n",
        "    # Binary labels for test data\n",
        "    y_test_binary = (y_test == class_num).astype(int)\n",
        "\n",
        "    # Convert test data to DMatrix\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    # Get predictions from the model\n",
        "    start_time = time.time()\n",
        "    y_pred_prob = model.predict(dtest)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Metric': [\n",
        "            'Accuracy',\n",
        "            'Balanced Accuracy',\n",
        "            'AUC-ROC',\n",
        "            'Matthews Correlation',\n",
        "            'Average Precision',\n",
        "            'Inference Time (ms)',\n",
        "        ],\n",
        "        'Value': [\n",
        "            accuracy_score(y_test_binary, y_pred_binary),\n",
        "            balanced_accuracy_score(y_test_binary, y_pred_binary),\n",
        "            roc_auc_score(y_test_binary, y_pred_prob),\n",
        "            matthews_corrcoef(y_test_binary, y_pred_binary),\n",
        "            average_precision_score(y_test_binary, y_pred_prob),\n",
        "            inference_time * 1000,\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Create a DataFrame for metrics\n",
        "    metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "    print(f\"\\n===== Performance Metrics for Class {class_num} =====\")\n",
        "    print(metrics_df.set_index('Metric').round(4))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_test_binary, y_pred_binary)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Other', f'Class {class_num}'],\n",
        "                yticklabels=['Other', f'Class {class_num}'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix for Class {class_num}')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n===== Feature Importance for Class {class_num} =====\")\n",
        "\n",
        "    # 8. Feature importance\n",
        "    feature_importance(model, class_num, feature_names)\n",
        "\n",
        "    return metrics_df, y_pred_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYjlXkh8O4hw"
      },
      "source": [
        "### Train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SRcDIHINM3S1",
        "outputId": "1252e75f-3a81-41f3-e8a0-4cf18a06b83d"
      },
      "outputs": [],
      "source": [
        "# Create arrays to store models and predictions\n",
        "models = []\n",
        "all_class_predictions = np.zeros((len(y_test), len(np.unique(y_train))))\n",
        "\n",
        "# For each class, train a binary classifier\n",
        "for num_class in range(len(np.unique(y_train))):\n",
        "    print(f'Training classifier for Class {num_class}')\n",
        "\n",
        "    # Create binary labels for ALL training examples\n",
        "    # 1 for current class, 0 for all other classes\n",
        "    y_train_binary = (y_train == num_class).astype(int)\n",
        "    y_val_binary = (y_val == num_class).astype(int)\n",
        "\n",
        "\n",
        "    # Create DMatrix objects\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train_binary)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val_binary)\n",
        "\n",
        "    # Counte num positive/negative examples for this class\n",
        "    pos_class = np.sum(y_train_binary == 1)\n",
        "    neg_class = np.sum(y_train_binary == 0)\n",
        "\n",
        "    # Train model\n",
        "    model = xgb.train(\n",
        "        params = {\n",
        "            'device': 'cuda',\n",
        "            'objective': 'binary:logistic',\n",
        "            'eval_metric': 'auc',\n",
        "            'max_depth': 10,\n",
        "            'learning_rate': 0.1,\n",
        "            'scale_pos_weight': neg_class / pos_class if pos_class > 0 else 1.0,  # Handle class imbalance\n",
        "            'seed': 42\n",
        "        },\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=2500,\n",
        "        evals=[(dval, 'validation')],\n",
        "        early_stopping_rounds=20,\n",
        "        verbose_eval=100\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    os.makedirs(os.path.join(path, 'models'), exist_ok=True)\n",
        "    model.save_model(os.path.join(path, f'models/xgboost_model_class_{num_class}.json'))\n",
        "\n",
        "    evaluate_model(model, X_test, y_test, num_class, feature_names)\n",
        "\n",
        "    # Store the model\n",
        "    models.append(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17QN4HLBYbje"
      },
      "source": [
        "## Combine models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "twpG0iZCYbjf",
        "outputId": "1e795072-4d76-407f-a8bb-fffdf4884570"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "class UnifiedXGBoostEnsemble:\n",
        "    \"\"\"A unified ensemble that handles multiple binary XGBoost models together\"\"\"\n",
        "\n",
        "    def __init__(self, models_dir):\n",
        "        \"\"\"Load all models from the specified directory\"\"\"\n",
        "        self.models = []\n",
        "        self.models_dir = models_dir\n",
        "        self.class_count = 0\n",
        "\n",
        "        for filename in sorted(os.listdir(models_dir)):\n",
        "            if filename.startswith('xgboost_model_class_') and filename.endswith('.json'):\n",
        "                model_path = os.path.join(models_dir, filename)\n",
        "                model = xgb.Booster()\n",
        "                model.load_model(model_path)\n",
        "                self.models.append(model)\n",
        "                self.class_count += 1\n",
        "\n",
        "        print(f\"Loaded {self.class_count} models from {models_dir}\")\n",
        "\n",
        "    def predict(self, X, strategy='max_prob'):\n",
        "        \"\"\"Make predictions using all models\"\"\"\n",
        "        # Convert to DMatrix for XGBoost\n",
        "        dtest = xgb.DMatrix(X)\n",
        "\n",
        "        # Get predictions from each model\n",
        "        class_probs = []\n",
        "        for model in self.models:\n",
        "            probs = model.predict(dtest)\n",
        "            class_probs.append(probs)\n",
        "\n",
        "        # Stack predictions into a matrix (samples x classes)\n",
        "        all_probs = np.column_stack(class_probs)\n",
        "\n",
        "        # Apply chosen combination strategy\n",
        "        if strategy == 'max_prob':\n",
        "            # Simply take class with highest probability\n",
        "            y_pred = np.argmax(all_probs, axis=1)\n",
        "        elif strategy == 'threshold':\n",
        "            # Apply threshold-based approach\n",
        "            binary_decisions = all_probs > 0.5\n",
        "            models_triggered = np.sum(binary_decisions, axis=1)\n",
        "\n",
        "            y_pred = np.zeros(len(all_probs), dtype=int)\n",
        "\n",
        "            # Case 1: Only one model predicted positive\n",
        "            single_model_mask = (models_triggered == 1)\n",
        "            if np.any(single_model_mask):\n",
        "                single_indices = np.where(single_model_mask)[0]\n",
        "                for idx in single_indices:\n",
        "                    y_pred[idx] = np.argmax(binary_decisions[idx])\n",
        "\n",
        "            # Case 2: Multiple models predicted positive\n",
        "            multi_model_mask = (models_triggered > 1)\n",
        "            if np.any(multi_model_mask):\n",
        "                multi_indices = np.where(multi_model_mask)[0]\n",
        "                for idx in multi_indices:\n",
        "                    y_pred[idx] = np.argmax(all_probs[idx])\n",
        "\n",
        "            # Case 3: No model predicted positive\n",
        "            no_model_mask = (models_triggered == 0)\n",
        "            if np.any(no_model_mask):\n",
        "                no_indices = np.where(no_model_mask)[0]\n",
        "                for idx in no_indices:\n",
        "                    y_pred[idx] = np.argmax(all_probs[idx])\n",
        "\n",
        "        return y_pred, all_probs\n",
        "\n",
        "\n",
        "def evaluate_unified_model(model, X_test, y_test, class_names=None):\n",
        "    \"\"\"\n",
        "    Evaluate the unified model's performance with comprehensive metrics\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model: UnifiedXGBoostEnsemble\n",
        "        The unified model to evaluate\n",
        "    X_test: numpy array\n",
        "        Test features\n",
        "    y_test: numpy array\n",
        "        Test labels\n",
        "    class_names: list\n",
        "        Names for the classes\n",
        "    \"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f\"Class {i}\" for i in range(model.class_count)]\n",
        "\n",
        "    # Try different combination strategies\n",
        "    strategies = ['max_prob', 'threshold']\n",
        "    results = {}\n",
        "\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n===== Ensemble Strategy: {strategy} =====\")\n",
        "\n",
        "        # Get predictions\n",
        "        y_pred, all_probs = model.predict(X_test, strategy=strategy)\n",
        "\n",
        "        # Basic metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "        mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "        print(f\"Overall Accuracy: {acc:.4f}\")\n",
        "        print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "        print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "        # Calculate ROC AUC and Average Precision (one-vs-rest approach)\n",
        "        roc_auc_scores = []\n",
        "        avg_precision_scores = []\n",
        "\n",
        "        for class_idx in range(model.class_count):\n",
        "            # Create binary labels (1 for current class, 0 for others)\n",
        "            y_true_binary = (y_test == class_idx).astype(int)\n",
        "            y_score = all_probs[:, class_idx]\n",
        "\n",
        "            # ROC AUC\n",
        "            try:\n",
        "                roc_auc = roc_auc_score(y_true_binary, y_score)\n",
        "                roc_auc_scores.append(roc_auc)\n",
        "            except:\n",
        "                roc_auc_scores.append(np.nan)\n",
        "\n",
        "            # Average Precision\n",
        "            try:\n",
        "                avg_precision = average_precision_score(y_true_binary, y_score)\n",
        "                avg_precision_scores.append(avg_precision)\n",
        "            except:\n",
        "                avg_precision_scores.append(np.nan)\n",
        "\n",
        "        # Print class-specific scores in a table\n",
        "        metrics_df = pd.DataFrame({\n",
        "            'Class': class_names,\n",
        "            'ROC AUC': roc_auc_scores,\n",
        "            'Avg Precision': avg_precision_scores\n",
        "        })\n",
        "\n",
        "        print(\"\\nPer-class metrics:\")\n",
        "        print(metrics_df)\n",
        "\n",
        "        # Calculate macro average\n",
        "        print(f\"\\nMacro-average ROC AUC: {np.nanmean(roc_auc_scores):.4f}\")\n",
        "        print(f\"Macro-average Avg Precision: {np.nanmean(avg_precision_scores):.4f}\")\n",
        "\n",
        "        # Classification report\n",
        "        report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(report)\n",
        "\n",
        "        # Confusion matrix\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'Confusion Matrix - {strategy} strategy')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Store all results\n",
        "        results[strategy] = {\n",
        "            'accuracy': acc,\n",
        "            'balanced_accuracy': balanced_acc,\n",
        "            'matthews_correlation': mcc,\n",
        "            'roc_auc': roc_auc_scores,\n",
        "            'avg_precision': avg_precision_scores,\n",
        "            'macro_roc_auc': np.nanmean(roc_auc_scores),\n",
        "            'macro_avg_precision': np.nanmean(avg_precision_scores),\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': all_probs\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Usage example\n",
        "models_dir = '/content/drive/MyDrive/models'\n",
        "class_names = [\"HBOND\", \"VDW\", \"PIPISTACK\", \"IONIC\", \"PICATION\", \"SSBOND\", \"PIHBOND\", \"Unclassified\"]\n",
        "\n",
        "# Create the unified model\n",
        "unified_model = UnifiedXGBoostEnsemble(models_dir)\n",
        "\n",
        "# Evaluate on test data\n",
        "results = evaluate_unified_model(unified_model, X_test, y_test, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzg9nhnTYbjf"
      },
      "source": [
        "## Pruning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pEsbHa9Kyzz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_xgboost_model(model_path):\n",
        "    \"\"\"Analyze an XGBoost model JSON file and extract useful information.\"\"\"\n",
        "    with open(model_path, 'r') as f:\n",
        "        model_data = json.load(f)\n",
        "\n",
        "    # Extract basic model information\n",
        "    results = {}\n",
        "\n",
        "    # Model metadata and version\n",
        "    results['version'] = '.'.join(map(str, model_data.get('learner', {}).get('version', [\"unknown\"])))\n",
        "\n",
        "    # Model attributes (parameters)\n",
        "    attributes = model_data.get('learner', {}).get('attributes', {})\n",
        "    results['best_iteration'] = int(attributes.get('best_iteration', -1))\n",
        "    results['best_score'] = float(attributes.get('best_score', -1))\n",
        "\n",
        "    # Extract hyperparameters\n",
        "    for key in ['max_depth', 'learning_rate', 'objective', 'eval_metric', 'num_class',\n",
        "                'scale_pos_weight', 'seed', 'num_parallel_tree', 'subsample', 'colsample_bytree']:\n",
        "        if key in attributes:\n",
        "            try:\n",
        "                value = attributes[key]\n",
        "                # Convert numeric strings to proper types\n",
        "                if isinstance(value, str) and value.replace('.', '', 1).isdigit():\n",
        "                    if '.' in value:\n",
        "                        results[key] = float(value)\n",
        "                    else:\n",
        "                        results[key] = int(value)\n",
        "                else:\n",
        "                    results[key] = value\n",
        "            except:\n",
        "                results[key] = attributes[key]\n",
        "\n",
        "    # Extract tree information\n",
        "    tree_model = model_data.get('learner', {}).get('gradient_booster', {}).get('model', {})\n",
        "\n",
        "    # Tree model parameters\n",
        "    gbtree_params = tree_model.get('gbtree_model_param', {})\n",
        "    results['num_trees'] = int(gbtree_params.get('num_trees', 0))\n",
        "\n",
        "    # Trees analysis\n",
        "    trees = tree_model.get('trees', [])\n",
        "\n",
        "    # Tree statistics\n",
        "    tree_stats = []\n",
        "    total_nodes = 0\n",
        "    total_leaves = 0\n",
        "    used_features = set()\n",
        "    feature_importance = defaultdict(int)\n",
        "\n",
        "    for i, tree in enumerate(trees):\n",
        "        tree_param = tree.get('tree_param', {})\n",
        "        num_nodes = int(tree_param.get('num_nodes', 0))\n",
        "        split_indices = tree.get('split_indices', [])\n",
        "\n",
        "        # Calculate leaves (nodes that are not internal nodes)\n",
        "        internal_nodes = set()\n",
        "        for j in range(len(tree.get('left_children', []))):\n",
        "            if tree['left_children'][j] >= 0:  # Not a leaf\n",
        "                internal_nodes.add(j)\n",
        "            if tree['right_children'][j] >= 0:  # Not a leaf\n",
        "                internal_nodes.add(j)\n",
        "\n",
        "        num_leaves = num_nodes - len(internal_nodes)\n",
        "\n",
        "        # Track feature usage as a simple feature importance\n",
        "        for feature_idx in split_indices:\n",
        "            used_features.add(feature_idx)\n",
        "            feature_importance[feature_idx] += 1\n",
        "\n",
        "        # Store tree statistics\n",
        "        tree_stats.append({\n",
        "            'tree_index': i,\n",
        "            'num_nodes': num_nodes,\n",
        "            'num_leaves': num_leaves,\n",
        "            'max_depth': max_tree_depth(tree),\n",
        "            'num_features_used': len(set(split_indices))\n",
        "        })\n",
        "\n",
        "        total_nodes += num_nodes\n",
        "        total_leaves += num_leaves\n",
        "\n",
        "    results['total_nodes'] = total_nodes\n",
        "    results['total_leaves'] = total_leaves\n",
        "    results['avg_nodes_per_tree'] = total_nodes / results['num_trees'] if results['num_trees'] > 0 else 0\n",
        "    results['avg_leaves_per_tree'] = total_leaves / results['num_trees'] if results['num_trees'] > 0 else 0\n",
        "    results['num_features_used'] = len(used_features)\n",
        "\n",
        "    # Sort feature importance\n",
        "    results['top_features'] = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    # Compute tree depth statistics\n",
        "    depths = [stat['max_depth'] for stat in tree_stats]\n",
        "    results['min_tree_depth'] = min(depths) if depths else 0\n",
        "    results['max_tree_depth'] = max(depths) if depths else 0\n",
        "    results['avg_tree_depth'] = sum(depths) / len(depths) if depths else 0\n",
        "\n",
        "    # File metrics\n",
        "    results['file_size_kb'] = os.path.getsize(model_path) / 1024\n",
        "\n",
        "    return results, tree_stats\n",
        "\n",
        "def max_tree_depth(tree):\n",
        "    \"\"\"Calculate the maximum depth of a tree by following child nodes.\"\"\"\n",
        "    left = tree.get('left_children', [])\n",
        "    right = tree.get('right_children', [])\n",
        "\n",
        "    if not left or not right:\n",
        "        return 0\n",
        "\n",
        "    # Use BFS to find the maximum depth\n",
        "    depths = {0: 0}  # node_id: depth\n",
        "    max_depth = 0\n",
        "\n",
        "    for node_id in range(len(left)):\n",
        "        node_depth = depths.get(node_id, 0)\n",
        "\n",
        "        # Process left child\n",
        "        if left[node_id] >= 0:  # Valid node\n",
        "            depths[left[node_id]] = node_depth + 1\n",
        "            max_depth = max(max_depth, node_depth + 1)\n",
        "\n",
        "        # Process right child\n",
        "        if right[node_id] >= 0:  # Valid node\n",
        "            depths[right[node_id]] = node_depth + 1\n",
        "            max_depth = max(max_depth, node_depth + 1)\n",
        "\n",
        "    return max_depth\n",
        "\n",
        "# Usage example\n",
        "models_dir = '/Users/sebastianosanson/Development/Contacts-Classification/models'\n",
        "all_model_results = []\n",
        "all_tree_stats = []\n",
        "\n",
        "# Process all model files\n",
        "for filename in sorted(os.listdir(models_dir)):\n",
        "    if filename.endswith('.json'):\n",
        "        model_path = os.path.join(models_dir, filename)\n",
        "        class_num = int(filename.split('_')[-1].split('.')[0])\n",
        "\n",
        "        # Extract model information\n",
        "        model_info, tree_stats = analyze_xgboost_model(model_path)\n",
        "        model_info['class'] = class_num\n",
        "\n",
        "        # Add model information to results\n",
        "        all_model_results.append(model_info)\n",
        "\n",
        "        # Add tree statistics with model identifier\n",
        "        for stat in tree_stats:\n",
        "            stat['class'] = class_num\n",
        "            all_tree_stats.append(stat)\n",
        "\n",
        "# Create DataFrame for easy analysis\n",
        "models_df = pd.DataFrame(all_model_results)\n",
        "trees_df = pd.DataFrame(all_tree_stats)\n",
        "\n",
        "# Print the high-level model information\n",
        "print(\"\\n===== MODEL INFORMATION =====\")\n",
        "print(models_df[['class', 'num_trees', 'best_iteration', 'best_score',\n",
        "                 'total_nodes', 'avg_nodes_per_tree', 'avg_tree_depth',\n",
        "                 'file_size_kb']].sort_values('class'))\n",
        "\n",
        "# Print tree depth statistics\n",
        "print(\"\\n===== TREE DEPTH STATISTICS =====\")\n",
        "tree_depth_stats = trees_df.groupby('class').agg({\n",
        "    'max_depth': ['min', 'max', 'mean']\n",
        "}).reset_index()\n",
        "print(tree_depth_stats)\n",
        "\n",
        "# Feature importance across models\n",
        "print(\"\\n===== TOP FEATURES BY CLASS =====\")\n",
        "for i, model in enumerate(all_model_results):\n",
        "    print(f\"\\nClass {model['class']} top features:\")\n",
        "    for feature_idx, count in model['top_features'][:]:\n",
        "        print(f\"  Feature {feature_names[feature_idx]}: used {count} times\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfKTs969Kyz0"
      },
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
        "# from sklearn.metrics import matthews_corrcoef, average_precision_score\n",
        "# import matplotlib.pyplot as plt\n",
        "# import time\n",
        "\n",
        "# def evaluate_model_comparison(original_model_path, pruned_model_path, X_test, y_test, class_num):\n",
        "#     \"\"\"\n",
        "#     Compare performance between original and pruned XGBoost models.\n",
        "\n",
        "#     Parameters:\n",
        "#     -----------\n",
        "#     original_model_path: str\n",
        "#         Path to the original model file\n",
        "#     pruned_model_path: str\n",
        "#         Path to the pruned model file\n",
        "#     X_test: numpy array\n",
        "#         Test features\n",
        "#     y_test: numpy array\n",
        "#         Test labels\n",
        "#     class_num: int\n",
        "#         Class number for binary evaluation\n",
        "#     \"\"\"\n",
        "#     # 1. Load both models\n",
        "#     original_model = xgb.Booster()\n",
        "#     original_model.load_model(original_model_path)\n",
        "\n",
        "#     pruned_model = xgb.Booster()\n",
        "#     pruned_model.load_model(pruned_model_path)\n",
        "\n",
        "#     # 2. Convert test data to DMatrix\n",
        "#     dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "#     # 3. Make predictions\n",
        "#     # Convert to binary task (1 for current class, 0 for other classes)\n",
        "#     y_test_binary = (y_test == class_num).astype(int)\n",
        "\n",
        "#     # Get predictions from both models\n",
        "#     start_time = time.time()\n",
        "#     y_pred_orig = original_model.predict(dtest)\n",
        "#     orig_time = time.time() - start_time\n",
        "\n",
        "#     start_time = time.time()\n",
        "#     y_pred_pruned = pruned_model.predict(dtest)\n",
        "#     pruned_time = time.time() - start_time\n",
        "\n",
        "#     # 4. Convert probabilities to binary predictions\n",
        "#     y_pred_orig_binary = (y_pred_orig > 0.5).astype(int)\n",
        "#     y_pred_pruned_binary = (y_pred_pruned > 0.5).astype(int)\n",
        "\n",
        "#     # 5. Calculate metrics\n",
        "#     metrics = {\n",
        "#         'Model': ['Original', 'Pruned'],\n",
        "#         'Accuracy': [\n",
        "#             accuracy_score(y_test_binary, y_pred_orig_binary),\n",
        "#             accuracy_score(y_test_binary, y_pred_pruned_binary)\n",
        "#         ],\n",
        "#         'Balanced Accuracy': [\n",
        "#             balanced_accuracy_score(y_test_binary, y_pred_orig_binary),\n",
        "#             balanced_accuracy_score(y_test_binary, y_pred_pruned_binary)\n",
        "#         ],\n",
        "#         'AUC-ROC': [\n",
        "#             roc_auc_score(y_test_binary, y_pred_orig),\n",
        "#             roc_auc_score(y_test_binary, y_pred_pruned)\n",
        "#         ],\n",
        "#         'Matthews Correlation': [\n",
        "#             matthews_corrcoef(y_test_binary, y_pred_orig_binary),\n",
        "#             matthews_corrcoef(y_test_binary, y_pred_pruned_binary)\n",
        "#         ],\n",
        "#         'Average Precision': [\n",
        "#             average_precision_score(y_test_binary, y_pred_orig),\n",
        "#             average_precision_score(y_test_binary, y_pred_pruned)\n",
        "#         ],\n",
        "#         'Inference Time (ms)': [\n",
        "#             orig_time * 1000,\n",
        "#             pruned_time * 1000\n",
        "#         ]\n",
        "#     }\n",
        "\n",
        "#     # Calculate file size\n",
        "#     import os\n",
        "#     metrics['Model Size (KB)'] = [\n",
        "#         os.path.getsize(original_model_path) / 1024,\n",
        "#         os.path.getsize(pruned_model_path) / 1024\n",
        "#     ]\n",
        "\n",
        "#     # 6. Create a DataFrame for metrics\n",
        "#     metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "#     # 7. Calculate performance difference\n",
        "#     diff_row = {\n",
        "#         'Model': 'Difference (%)',\n",
        "#         'Accuracy': (metrics['Accuracy'][1] - metrics['Accuracy'][0]) / metrics['Accuracy'][0] * 100,\n",
        "#         'Balanced Accuracy': (metrics['Balanced Accuracy'][1] - metrics['Balanced Accuracy'][0]) / metrics['Balanced Accuracy'][0] * 100,\n",
        "#         'AUC-ROC': (metrics['AUC-ROC'][1] - metrics['AUC-ROC'][0]) / metrics['AUC-ROC'][0] * 100,\n",
        "#         'Matthews Correlation': (metrics['Matthews Correlation'][1] - metrics['Matthews Correlation'][0]) / max(0.0001, metrics['Matthews Correlation'][0]) * 100,\n",
        "#         'Average Precision': (metrics['Average Precision'][1] - metrics['Average Precision'][0]) / metrics['Average Precision'][0] * 100,\n",
        "#         'Inference Time (ms)': (metrics['Inference Time (ms)'][1] - metrics['Inference Time (ms)'][0]) / metrics['Inference Time (ms)'][0] * 100,\n",
        "#         'Model Size (KB)': (metrics['Model Size (KB)'][1] - metrics['Model Size (KB)'][0]) / metrics['Model Size (KB)'][0] * 100\n",
        "#     }\n",
        "#     metrics_df = pd.concat([metrics_df, pd.DataFrame([diff_row])], ignore_index=True)\n",
        "\n",
        "#     print(f\"\\n===== Performance Comparison for Class {class_num} =====\")\n",
        "#     print(metrics_df.round(4))\n",
        "\n",
        "#     # 8. Plot prediction correlation\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.scatter(y_pred_orig, y_pred_pruned, alpha=0.3)\n",
        "#     plt.plot([0, 1], [0, 1], 'r--')\n",
        "#     plt.xlabel('Original Model Predictions')\n",
        "#     plt.ylabel('Pruned Model Predictions')\n",
        "#     plt.title(f'Prediction Correlation for Class {class_num}')\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "#     correlation = np.corrcoef(y_pred_orig, y_pred_pruned)[0, 1]\n",
        "#     plt.text(0.05, 0.95, f'Correlation: {correlation:.4f}', transform=plt.gca().transAxes)\n",
        "#     plt.show()\n",
        "\n",
        "#     return metrics_df\n",
        "\n",
        "# # Example usage\n",
        "# # class_num = 0  # For HBOND class\n",
        "# # original_path = '/Users/sebastianosanson/Development/Contacts-Classification/models/xgboost_model_class_0.json'\n",
        "# # pruned_path = '/Users/sebastianosanson/Development/Contacts-Classification/models/xgboost_model_class_0_pruned.json'\n",
        "# # results = evaluate_model_comparison(original_path, pruned_path, X_test, y_test, class_num)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sb_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
